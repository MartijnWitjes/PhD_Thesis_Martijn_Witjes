\chapter[Synthesis]{Synthesis}
\label{cha:Chapter6}
\newpage

This thesis has contributed towards efforts in large-scale land cover mapping, with an emphasis on the benefits of combining several datasets from different sources and of different types.  It presents different steps of a methodology to extract training data from multiple rich human-annotated datasets and overlay them on Earth observation data from diverse sources, and details all the challenges and benefits of doing so while creating land cover maps that navigate the trade-off between spatial, temporal, and thematic resolution, as well as quantity and allocation accuracy.

This thesis, especially in its first two chapters, summarises a relatively 'applied' line of research. Its most important outputs are several datasets that were produced, and have been published as open data (CC-BY):
\begin{enumerate}
    \item 2000-2020 quarterly Landsat composites at 30m resolution and 7 bands;
    \item 2016-2019 quarterly/annual Sentinel-2 composites at 10-30m resolution, depending on the band;
    \item An Ensemble Digital Terrain Model of Europe at 30m resolution;
    \item 2000-2019 annual Land Use / Land Cover maps of Europe at 30m resolution and 43 classes;
    \item Five annual land cover maps of five European countries (Belgium, Czechia, Germany, Luxembourg and The Netherlands) at 30m resolution and 8 classes, whose class proportions match Eurostat area estimates.
\end{enumerate}
The third (and fourth?) chapters describe the later and relatively more innovative attempts to make maps that are faithful to area estimates, and attempts to improve the process of classifying many land cover types with minimal sacrifice of accuracy.

This chapter is divided into two sections: the first summarizes contributions to the research objectives formulated in Section~\ref{sec:research_objectives}; the second reflects on these contributions, offering perspectives on future research opportunities. 

\section{Main Findings}
    The chapters of this thesis each address multiple research questions and have considerable overlap. Their results will be discussed from the perspective of the research questions; each question is discussed below.
    
    \subsection{What are the benefits and challenges of combining multiple large time-series and static EO datasets into Analysis-Ready Data for the purpose of land cover classification?}

This research question was largely explored in chapters 2 and 3. Chapter 2 details the construction of an Earth observation data cube from several sources (Landsat, Sentinel-2 and several DTMs), and includes experiments that investigate the performance of land cover models using different combinations of these datasets. Chapter 3 uses most of this data cube to train a single LULC classification model, which is subsequently used to create annual maps of Europe between 2000 and 2019. Both chapters include experiments that investigate the generalization potential of models trained on data from a single year, and those trained on data from several years.

    \subsubsection{Benefits of combining datasets}

        Our land cover experiments in chapter 2 show that random forest models trained on the largest combination of datasets achieved the highest classification accuracy during cross-validation and that models trained on Landsat and DTM data achieved the highest accuracy on test data.
        [This means that combining data from multiple sources helps make better maps]
        [What does it mean that the larger feature space performed better during CV but that this was not the case on test accuracy?]

        Variable importance of chapter 3 shows that different variables were used by different models

        We created an Ensemble DTM which was more accurate than its four source datasets: RMSE 6.544 vs RMSE of 8.451-9.900. [This shows that using different maps as input can be used to produce a more accurate map.]
        [Does this also apply to a classification problem?]

        

    \subsubsection{Challenges of combining datasets}
        
        Experiments in Chapter 2\ref{cha:chapter2} showed that models using the full feature space (Landsat, Sentinel-2 and DTM) achieved the highest classification accuracy, with different datasets improving the results for different land cover classes. This is supported by variable importance in Chapter \ref{cha:chapter3}: Data from four different sources (Landsat, surface water frequency, DTM, and distance to coast) were among the top 15 of 200 variables for LULC modeling.
        
        While it is clear that combining different datasets into one feature space can improve model performance, there are some challenges:
        \begin{itemize}
        \item Differences in spatial resolution
        \item Differences in temporal resolution
        \item Missing data
        \end{itemize}

        \textbf{Spatial resolution}
        Overlaying different raster datasets onto training samples may yield high accuracy (like in chapter 2), but can cause artifacts in the shape of low-resolution raster cells creating maps at the highest resolution among the used datasets \citep{moller2019oblique} [SNOWBALL A BIT HERE], or cause low mapping accuracy when mapping at a lower resolution [CITE]. 
        
        \textbf{Temporal resolution}:   
        Which timesteps do you map? Some maps are made by aggregating and averaging observations from several years \citep{pflugmacher2019mapping,esa2023cci}, which may give more robust results than annual maps, but loses detail. In chapters 3 and 4, we chose to make annual maps, and in chapter 3, we made annual maps for each consecutive year. 
    
        We used a space-first predictions strategy throughout this thesis, and not a time-first strategy [CITE GILBERTO?]. This has benefits: [BENEFITS] but also drawbacks. For example, mapping classes that change inside a year, such as crops.
        
        \textbf{Missing Data}:
        This was also seen in chapter 3: While we smoothed out MODIS layers when subsampling to a 30m resolution to avoid artifacts from spatial resolution differences, the land cover maps have gaps near some coastlines due to missing auxiliary layers (see an example \href{https://ecodatacube.eu/?base=OpenStreetMap%20(grayscale)&layer=Land%20Cover%20&zoom=13.2&eye=5000000&center=52.4582,5.3267&opacity=100&time=2019}{here}). We only noticed this much later
    
    
    \subsection{To what extent does training data from multiple times and places improve the accuracy and generalization of land cover classification?}
    
    We did not have training data for each year, and therefore needed to test how well our model could generalize to 'unseen' years. Our results show that models trained on samples from different years were more accurate; both on years with available training data, and on unseen years. In particular, our map of 2017 was validated on the points collected by \citep{jenerowicz2021validation} to validate the S2GLC maps made by \citet{malinowski2020automated}, and achieved a similar accuracy without having training data from 2017 and operating on a lower spatial resolution.
    
    In Chapter \ref{cha:chapter3}, we combined LUCAS points with samples extracted from CORINE polygons to create a training dataset with samples from X years. Cross-validation showed that the accuracy of the model was more consistent through time than through space
    
    \subsection{How does the number and type of classes in a legend affect the accuracy of land cover classification?}
    
    \subsubsection{Number of classes}
    
    
            eml lulc        hsc lc
            f1      n       f1  n
    lvl1    0.83    5       X   8
    lvl2    0.51    14      X   X
    lvl3    0.49    43      X   52
    
    
    
    \subsubsection{Type of classes}
    
    LULC
    
    different land use
    pasture / natural grassland / airports
    
    from eml lulc
    
    aggregating the probabilities predicted by ensemble models 
    When we summed the probabilities of all classes that composed the S2GLC legend in chapter \ref{cha:chapter3}, we achieved similar or higher accuracy than the S2GLC land cover maps. This means that training a model on many classes, even more classes than 'needed' for a specific use case, does not need to harm its accuracy in a simpler legend.
    
    \subsection{What is the effect of enforcing correct class proportions on map accuracy?}
    
    from IMP
    
    \subsection{Which uncertainty metric is the most useful for optimizing the trade-off between detail and accuracy?}

    \subsection{Other findings}
    We also have findings that were not part of the research questions:

    
    
\section{TMP: IDEAS}

mention EAGLE legend

mention direct area estimation? conformal prediction? uncertainty? Can of worms

mention cool suppliers of open data:
Major TOM \citep{francis2024major}



\section{Reflection and outlook}

    \subsection{Reflection: do we really need these maps? What kind of maps do we really need?}

    maps without users are meaningless
    wrong maps with users are dangerous \citep{bastin2019global}
    good maps with the wrong users are even more dangerous - soil carbon, investors want to keep polluting. Disaster analysis maps are used by investors and insurance companies, buying land. Poachers etc. Do farmers benefit from cropland maps? No.
    Exponential benefit to those who are already rich and powerful.
    
    Spatial resolution:
        
    temporal resolution:
    crop mapping annually is difficult: you need sub-annual maps to do it properly. But where does this end? Daily maps?
    
    \subsection{Iterative Mapping of Probabilities}

        The IMP algorithm was developed as an attempt to produce maps that match existing area estimates without sacrificing accuracy. We discovered that it can \textit{improve} the accuracy, especially of biased models. The ability of IMP to correct the bias of a model post-hoc also means that models don't need to be trained on datasets that respect class balance. This means it can be combined well with training data generation techniques that don't respect class distributions, such as extracting points from polygons (see chapters 3, 4 and 5). Furthermore, it might improve the usefulness of oversampling techniques such as SMOTE  \citep{chawla2002smote}. While popular for improving minority probability predictions \citep{elor2022smote} and actively researched \citep{douzas2019imbalanced}, SMOTE is critiqued for mostly helping weak classifiers and actively harming the performance of stronger classifiers \citep{elor2022smote}. Oversampling improves the probability predictions of rare classes but worsens majority and overal log loss; this is not an issue when the probabilities are classified with IMP, as it only uses within-class probability distributions.
        
        With optimized thresholds, oversampling only helps very weak classifiers like MLP. Threshold optimization otherwise preferred \citep{elor2022smote} -> IMP can be considered such a threshold optimization technique!

        The skewed class distribution may not be the core reason for poor performance on imbalanced data. Other factors like small sample size may play a bigger role. \citep{elor2022smote} -> Methods like mine that can generate large training datasets are therefore nice with it!
        

        \subsubsection{Extrapolating area estimates}
            IMP learns the bias of the model, and quantifies it in the cut-off probability value for each iteration to make sure the bias of the model is countered. This might be generalizable to areas where you don't have area estimates, if the confusion between classes is similar.
            You could store the probability threshold for each class at each iteration, and apply them to probabilities predicted by the same model, for a different area.
            this can be explored by mapping neighboring NUTS2 areas of the ones we have already mapped, using the cutoff values of their originally mapped neighbor. We can then pixel count, and compare the counts to the area estimates.

            This could make the method applicable for areas that are less meticulously quantified than Europe.
        
        \subsubsection{Validating area estimates}
            If a reliable, trusted validation data exists for an area, IMP can be used to compare the accuracy of different area estimates by making hard-class maps for each area estimate. The area estimate that matches the reality on the ground most closely should allow IMP to produce the most accurate map.
            In this way, IMP could be used to settle disputes about quantities of land cover and land cover change.

            example: Forests

        \subsubsection{Wider applicability}
            Land cover and other remote sensing fields are not the only potential application for IMP. It can be used for any type of machine learning task where class proportions can be either estimated or targeted. Any field that utilizes machine learning models and faces challenges with class imbalance, representation bias, or requires models to perform accurately across diverse and potentially underrepresented groups would find such an algorithm highly beneficial. This approach could significantly enhance model fairness, accuracy, and generalization across a wide range of applications.

            In medical diagnosis and disease prediction models, data can be inherently biased due to uneven representation of disease cases across populations. An algorithm that corrects for such biases could improve the accuracy of diagnostic tools and predictive models, making them more reliable for clinical decision-making and research studies. For instance, it could help in developing more accurate models for rare disease diagnosis where the class imbalance is a significant issue \citep{weiss2004mining,krawczyk2016learning}.

            Financial institutions often use machine learning models for credit scoring and risk assessment. The training data can be biased due to historical decisions and social demographics, potentially leading to unfair assessments. A post-hoc correction algorithm could mitigate these biases, leading to fairer credit scoring and risk assessment models \citep{chen2018why, kamiran2012data}.

            In predictive policing and recidivism prediction, biases in the training data can perpetuate and amplify societal biases. An algorithm that adjusts predictions post-hoc could help in reducing the bias in such models, contributing to fairer decision-making in the criminal justice system \citep{berk2021fairness, dressel2018accuracy}.

            Recommendation systems in retail often suffer from popularity bias, where popular items are more likely to be recommended, potentially ignoring the long tail of less popular items. An algorithm that can correct for such biases post-hoc would improve the diversity and fairness of recommendations \citep{abdollahpouri2019managing}.
    
    
    \subsection{Hierarchical Selective Classification}
    
        Selective hierarchical classification will be more fine-grained if you have more levels in your hierarchy.
        Eurostat should publish more detailed area estimates and include LULC combinations. There is plenty of training data available for e.g. pastures, natural grasslands, and other grass areas such as those near airports.

        \subsubsection{Prediction uncertainty}
        
            Finding the best uncertainty metric will be helpful. Well-calibrated probabilities and the margin of victory \citep{calderon-loor2021high} can be used as a heuristic proxy but offer no statistical guarantee. The land cover community is exploring techniques that promise such statistical guarantees on uncertainty quantification, such as conformal prediction \citep{angelopoulos2023predictionpowered,valle2023quantifying,singh2024uncertainty}. So far, however, they remain unused by the large-scale mapping initiatives, although those that publish all probabilities (like DynamicWorld) allow users to calculate some metrics themselves [CITE who did that]
        
            IMP: Classifications in earlier iterations are much more accurate than later classifications. The user accuracy on calibration data (from the same source as the training data) is a robust indicator of relative accuracy, which then can be regularized using the final validation of the map with an independent test dataset (e.g. LUCAS).
    
        \subsubsection{Best models for the job}
                Can you train models in such a way that they are relatively more likely to make mistakes within the same category? Iterative learning models, such as gradient boosting and neural networks, might be trained with custom loss functions.

    \subsection{Transfer learning and Foundational Models}
    
    
    
    \subsection{Open Data}
    https://radiant.earth/blog/2023/05/we-dont-talk-about-open-data/ (the case against open data: helping poachers etc.)
    \subsection{Future maps}
    
        People are working on reproducing CORINE with ML, allbeit at 14 classes \citep{bhugra2022rapidai4eo}
        
        It is important to validate annual large-scale maps with up-to date validation data to ensure there is no dataset drift and to properly quantify accuracy and uncertainty \citep{tsendbazar2021towards}
    
    
            
                
            



        

