\chapter[Introduction]{Introduction}
\label{cha:Chapter1}
\vspace*{\fill}


\newpage

\section{Context}


\subsection*{Land cover}

    Land cover, or the physical and biological material found on the surface of the earth, encompasses natural environments like forests, grasslands, wetlands, and things made by humans, like urban areas, croplands, and infrastructure. 
    
    Its study and monitoring are crucial for a multitude of reasons. Land cover changes significantly influence regional and global climate patterns and are a major driver of biodiversity loss, emphasizing the necessity of monitoring for climate change mitigation and biodiversity preservation \citep{pielke2002influence, houghton2012carbon, sala2000global, cbd2016indicators}. 
    
    Additionally, these changes have direct implications for the quality and availability of natural resources, such as water and air \citep{foley2005global}. Land cover assessment is vital for evaluating the livability of human environments \citep{levering2024landscape} and has key applications in informing policy, analyzing land-based emissions, and estimating local climate extremes \citep{duveiller2020, hong2021luemissions, sy2020}. 
    
    Therefore, understanding land cover dynamics is crucial for effective policymaking at both regional and global levels \citep{liu2020assessing, trisurat2019land, shumba2020effectiveness}. As different people with different objectives require different maps with different things on them, the choice of what type of land cover to map is an important one. 

    \subsubsection*{Crops and Grasslands}

        Cropland and grassland are essential for food production, and together cover over 40\% of the European Union's territory \citep{eurostat2021land}. European cropland is a diverse land cover type: Over 100 crops are produced at scale in Europe, ranging from cereals such as wheat and rice to sunflowers, tobacco, and herbs such as basil and valerian \citep{eurostat2023agricultural}. Grasslands, on the other hand, cover wide roles beyond dairy and meat production: Water supply and flow regulation, carbon storage, erosion control, climate mitigation, pollination, and cultural significance \citep{bengtsson2019grasslands}. 
        
        The agricultural sector is not only a cornerstone of the EU's economy but also a significant employer, with approximately 4.2\% of the EU's population engaged in agriculture. Reflecting its importance, the European Union's Common Agricultural Policy (CAP) has earmarked €264 billion as agricultural subsidy for the 2023–2027 period. The economic magnitude of crop and livestock production makes tracking and understanding their dynamics in the EU a priority for many stakeholders.

    \subsubsection*{Forests}

        Forests cover nearly a third of the planet's land area \citep{fao2022,banskota2014forest} and more than 41\% of the EU \citep{eurostat2021land} and are important for biodiversity \citep{cazzolla2022number}, carbon sequestration \citep{ipcc2021}, and have been supplying a plethora of resources used by human civilization since the palaeolithic. Given recent trends of globalization and deforestation, monitoring forest cover change and its drivers is crucial \citep{sy2019tropical,masolele2024mapping} for a great number of actors, from the UN and national governments, the industrial sector, local communities, and Indigenous peoples.

        National forest inventories, statisticians, and mapping organizations often disagree about the quantity of forest in any given area, which can lead to fierce scientific debates \citep{picard2021recent, korhonen2020new, palahi2021concerns, rossi2019assessing}
    
    \subsubsection*{Wetlands}

        Wetlands, recognized as one of the most biologically productive and important natural ecosystems, play a pivotal role in regulating the global climate, maintaining the hydrological cycle, and supporting diverse species \citep{hu2017global,ramsar2001wetlands}. The unique hydrology of these ecosystems influences the distribution of sediment, nutrients, flora, and fauna. They provide essential services such as improving water quality, flood mitigation, recharging aquifers, and supporting an array of wildlife \citep{costanza1997value,smardon2009sustaining}. Despite these benefits, wetlands have historically been undervalued, often seen as wastelands or disease sources, leading to significant losses due to reclamation and degradation \citep{giblett1996postmodern,owens2001global,davidson2014much,gardner2018global,dugan1993wetlands,oecd1996guidelines,ramsar2015wetlands}. The world has witnessed a dramatic decline in wetland areas, with an estimated 87\% degradation since 1700, particularly in the 20th and early 21st centuries, resulting in substantial economic losses and reduced ecosystem service value \citep{gardner2015state,costanza2014changes}. While they only cover 1.7\% of the EU \citep{eurostat2021land}, their vital role and the alarming rate of loss makes it crucial to preserve their ecological integrity and ensuring the continued provision of their invaluable ecosystem services.

    \subsubsection{Land Use}

        Land Use is not just what's there, but how it is being used by humans. A grassland can be an intensively grazed pasture or a protected nature reserve. A forest can be a palm oil plantation, a recreational area, or a rare old growth forest with religious significance. The term Land Use is sometimes used interchangeably with Land Cover, and different authors make different distinctions. For example \citet{hansen2022global} group inland water and wetlands as land cover, and built-up area, cropland and tree cover change as land use. Sometimes, a legend contains classes that are combinations of Land Use / Land Cover (LULC), such as pastures and natural grasslands in CORINE land cover, or even the grass fields at airports in the LUISA Basemap of Europe \citep{pigaiani2021luisa}. Because such classes can be ambiguous and hard to map by computer programs, they are not often reproduced at scale with remote sensing techniques. The LUCAS survey makes a clear distinction between land use and land cover; each observation has a separate listing for both types, with some combinations occurring frequently, and others not at all.

    \subsubsection*{What should be on a map?}

        It should become clear now that there are a great many things that can be mapped. Large categories such as 'Forest' or 'Water' are relatively simple for people and computers to differentiate. However, adding more detail to the map by splitting up these big categories into more specific classes, or improving the \textit{thematic resolution}, brings many challenges. 
        
        First of all, maps with many classes are hard to read by humans. This is of course not always necessary, as users can derive simpler maps from rich datasets to tailor them to their use case. \citep{tsendbazar2017integrating}, or use the mapped values for a different type of analysis. 

        Secondly, the process of making detailed maps is more costly and difficult. You need to collect more examples from more different categories, and not all categories are equally easy to distinguish by surveyors. Someone interpreting aerial imagery might be able to distinguish grassland from forest, but what if they want to map different tree species, or distinguish pastures from natural grassland?

    Which brings us to the next question: How do you get those maps? People used to draw them based on field surveys. Later maps were based on aerial imagery. In recent decades, we have been increasingly using remote sensing and machine learning.

\subsection*{Land cover classification with machine learning and earth observation data}
    
    Land cover classification is the process of making land cover maps using automated processes. You need:
    
    \begin{itemize}
        \item \textbf{Examples:} Confirmed observations of different types of land cover, with a known time and place. We call this \textbf{reference data}.
        \item \textbf{Earth observation data:} satellite imagery and other geographic data that cover the area you want to map. Together, these \textbf{covariates} form the \textbf{feature space}. Spatial and temporal metadata allow aspiring mappers to combine several existing, unrelated datasets into one feature space; a rarity in the machine learning domain. 
        \item A \textbf{Model:} Some kind of decision-making algorithm that learns to recognize which examples often occur at which combination of the covariates. If this goes well, you can use it to \textbf{predict} which land cover occurs at locations where you don't have reference data, but which are covered by the feature space.
    \end{itemize}
    
    It is common to use different sets of reference data: One set to train your model, and one independent set to validate its predictions: The land cover maps it has made. It is good if these two datasets were collected separately from each other, because this helps to make sure your model \textbf{generalizes} well: That its predictions are accurate in new situations, not just on the data it was trained on.

    \subsubsection*{Training data}

        Training data is a set of labeled observations that are used to teach a machine learning model to recognize similar things in new situations. 
        
        The goal of most trained models is that they \textbf{generalize} well, which means that they can make consistent and accurate estimations in areas and moments they were not trained on. To make sure your model generalizes well, you have to cover the feature space of the area you want to map \citep{meyer2021predicting}. This does not directly relate to geographical space; large homogeneous areas such as deserts tend to be quite similar in the feature space, while small complex landscapes such as cities with parks, gardens and canals will show great local variation. 

        It is important to have enough training examples \citep{ramezan2021effects,rodriguez-perez2017influence,zhu2016optimizing}, but what is 'enough' differs per model  \citep{myburgh2014impact} and task \citep{koshute2021recommending}.
        
        Models often learn to estimate classes in the same proportions as their training data. If these proportions do not match the real-world situation of the area they need to map, the model is \textbf{biased}. To minimize this bias and predict the right relative quantities of each class, you need training data that matches the desired proportions \citep{he2009learning, kleinewillinghofer2022unbiased}.
        
    \subsubsection*{Validation data}

        All maps are wrong \citep{monmonier2018how}, and maps produced with machine learning and earth observation data are no different. Errors can come from various sources: Some classes might be hard to distinguish in the available feature space, confusing the model. Some classes might be overrepresented in the training data, leading the model to predict it more frequently at the expense of other classes. 

        Quantity and allocation: Are there enough pixels of every class, and are they on the right place? \citep{pontius2006can,pontius2011death}

        In general, it is essential that any map is validated with a set of observations. Such validation datasets need to be high quality: Their accuracy needs to be higher than the target accuracy of the map, they need to be representative of the classes in the mapped area 
    
        Bias in the training data can lead a model to overpredict certain classes. In this case, the proportions of the classes on the map will deviate from the proportions in the real world. It is very difficult to quantify the bias of a model before it is used \citep{stehman2013estimating}, and the current best way of fixing the proportions on a map is to measure the model bias with additional sampling after the mapping is complete \citep{stehman2014estimating}

        Classification errors become even more troublesome when land cover change is measured by comparing maps from two time periods, as any misclassified pixel will be interpreted as a change \citep{olofsson2013making}. This can be counteracted by careful sampling design \citep{stehman2012impact,olofsson2014good}, although some work has been done to circumvent this and derive area estimates directly from the model predictions \citep{sales2022land}
    
\subsection*{The current land cover mapping arms race}
    
    In the last few years, several large-scale land cover maps have been made.
    These maps often have high resolution and high accuracy, but are usually limited in multiple ways: Reproducibility, uncertainty, time coverage, and detail.
    
    \subsubsection{Reproducibility}
        The current big mapping initiatives to only share their maps, not the reference data that they used to produce them. This not only makes it difficult or impossible to reproduce, validate \citep{venter2022global}, or improve their work, but also prevents it from being used for new projects \citep{tsendbazar2015assessing}.  A notable exception: The training data for Google's DynamicWorld \citet{brown2022dynamic} is available \citep{tait2021dwtd}.
    
    \subsubsection{Uncertainty} 
        What is the chance that a pixel on the map is actually that class? This 'User accuracy' is often reported for the whole map, separately for each class, but does not take into account that some pixel predictions are more likely to be correct than others. \citet{calderon2021high} used a 'margin of victory' (The difference in predicted probability between the most likely and second most likely class) to quantify prediction certainty. More recently, the land cover community is exploring techniques that promise statistical guarantees on uncertainty quantification, such as conformal prediction \citep{angelopoulos2023predictionpowered,valle2023quantifying,singh2024uncertainty}. So far, however, they remain unused by the large-scale mapping initiatives, although those that publish all probabilities (like DynamicWorld) allow users to calculate some metrics themselves
    
    \subsubsection{Time coverage} 
        Most of the current generation of maps use 10m  Sentinel-2 data, which only became available since 2016. Mapping initiatives that use Landsat data can go back much further, but are then limited to mapping at 30m resolution. Examples are the Australian land cover time series made by \citep{calderon2021high} and the work of UMD GLAD, which produced annual land cover maps from the year 2000 onwards using Landsat data \citep{hansen2022global}.
    
    \subsubsection{Detail}
        They often depict around ten land cover categories (see Table~\ref{tab:bigmaps}) Mapping more classes is more difficult, especially for large diverse areas (e.g. continents or the whole planet). It is also more work to collect the data required to train and validate the models. [ CITE NANDIKA?]
    
    Still, in general, these products are all limited in more than one of these ways. 
    
    \begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|c}
    \hline
    \textbf{Mapping Product} & \textbf{Classes} & \textbf{Training Data} & \textbf{Validation Data} & Time Extent & Uncertainty \\ \hline
    ESA WorldCover \citep{zanaga2022esa}                    & 11    & No                                    & No                                    & two years         & No?           \\ 
    \hline
    Google DynamicWorld                                     & 9     & Yes \citep{tait2021dwtd}              & No                                    & 2016+, monthly    & Probabilities \\ 
    \hline
    S2GLC Maps of Europe \citep{malinowski2020automated}    & 13    & No                                    & Yes \citep{jenerowicz2021validation}  & One year (2017)   & No            \\ 
    \hline
    ESRI Land Cover                                         & 9     & No                                    & No Information                        & No                & No            \\
    \hline
    Pan-European land cover \citep{pflugmacher2019mapping}  & 12    & Yes \citet{dandrimont2020harmonised}  & Yes (cross-validation)                & One year          & No            \\ 
    \hline
    Australian Land Cover \citep{calderon2021high}          & 6     & No                                    & No Information                        & 1985-2015         & Margin of Victory \\
    \end{tabular}
    }%
    \caption{Summary of Mapping Products and Data Availability}
    \label{tab:bigmaps}
    \end{table}
    
    
    \subsection*{Rise of open data}
    
    Simultaneously as large governmental and private organisations flex their mapping muscle, a wealth of openly available data has been emerging. Funded by governments, companies, and philantropy - and sometimes sparked by rogue scientists (shoutout to gilberto camara) - more and more Earth observation data and land cover information is becoming openly available. While this rapidly growing landscape of increasingly large datasets on diverging platforms and systems can be hard to navigate \citep{wagemann2021a}, it has fuelled an exponential increase in the amount of mapping initiatives and environmental awareness \citep{wulder2022fifty}, from deforestation \citep{hansen2013high}, soil \citep{hengl2017soilgrids250m}
    
    Especially in Europe, long-running datasets such as CORINE land cover and the LUCAS survey have provided detailed and consistent land cover information for almost two decades. \citet{pflugmacher2019mapping} demonstrated that it is possible to make accurate maps only using the openly available LUCAS dataset published by \citet{dandrimont2020harmonised} as training data.


\section{Research Gaps}

    There is a reproducibility crisis in science, and environmental monitoring is no exception. How can decision-makers trust datasets whose creation does not only involve complex workflows, but can not be reproduced or improved? It turns out that they often don't, and keep relying on older methods that might be slow and expensive, but reproducible and reliable. The current boom in for-profit remote sensing has allowed for excellent analysis using high-resolution data (e.g. drivers of deforestation in Africa \citep{masolele2024mapping} using 3~m resolution Planet data), but limitations on sharing this data, and the profit and marketing motivations of corporations makes sharing, trusting, and using their maps difficult.

    Furthermore, 

    There are no large-scale maps with more than 13 classes.

    Pixel-based uncertainty is not commonly used for maps with high thematic resolution.





\section{Objectives}

The overall objective of this PhD thesis is to leverage the availability of various open European land use / land cover datasets and statistics, and to combine these with earth observation data and machine learning to make good maps. More specifically, this thesis aims to answer the following research questions:

\begin{enumerate}
    \item How can various EO datasets be combined to be good for land cover classification?
    \item To what extent does spatiotemporal approach lead to better results?
    \item How can allocation and quantity agreement be optimized?
    \item How can accuracy and detail be optimized?
\end{enumerate}



\section{Thesis Overview}

The research questions posed in the previous section are answered through four research papers, which are presented as chapters in this thesis. Figure~\ref{fig:01_conceptual_framework} provides a graphical overview of how each chapter interacts in a conceptual framework. The first two chapters focus on combining earth observation and land cover data from multiple sources to train models that can generalize well to unknown years. The second two chapters introduce the use of existing area estimates to optimize the accuracy, quantity, and detail of predictions by such models.

\begin{figure}
    \centering
    % \includegraphics{figs_01/fig_conceptual_framework.png}
    \caption{Caption}
    \label{fig:01_conceptual_framework}
\end{figure}




\textbf{Chapter 2} focuses on the benefits and challenges of harmonizing and combining large-scale spatiotemporal datasets for land cover mapping, most of which were used in the following chapters. The chapter details the work that went into creating, harmonizing, and imputing multiple Earth observation datasets (Landsat, Sentinel-2, and a new 30m resolution DTM) covering Europe. It introduces and describes the imputation algorithm TMWM that was used to impute the Landsat data, and validates its accuracy in a spatiotemporally explicit way. It then explores how combining the different datasets improves the accuracy of land cover classification models. Lastly, it shows that models trained on samples from a longer time range can generalize better to years that they have not been trained on.

\fullcite{witjes2023ecodatacube}

\textbf{Chapter 3} focuses on the production of annual land use / land cover maps of Europe for 2000-2020. It details the steps taken to harmonize and clean the training data from multiple openly available sources (CORINE, LUCAS) into a legend with 43 classes. A thorough accuracy assessment using cross-validation and an independent set of S2GLC validation points describes how well the model generalizes across space and time, and quantifies the trade-off imposed by having a legend with high thematic resolution. Results show that the maps have similar accuracy as other current continental-scale maps at low thematic resolution, and that a more detailed legend introduces more errors.

\fullcite{witjes2022spatiotemporal}

\textbf{Chapter 4} introduces IMP, an algorithm that uses land cover area estimates to iteratively classify land cover from existing probabilities, producing maps whose class proportions match the input estimates. It details the algorithm and showcases its use by mapping 5 European countries in 5 years. The accuracy of the maps is compared with maps created using highest likelihood classification. Results show that the proportional maps do not only have more accurate class proportions, but equal or better accuracy than highest likelihood maps. We also compare the accuracy and proportions of maps based on probabilities predicted by models trained on data representative of the area of interest, and probabilities predicted by a general model trained on large parts of Europe. Results show that maps based on general model predictions reach more accurate class proportions, while maps based on local model predictions are slightly more accurate. Finally, it presents an unintentional finding that the iterations at which the algorithm classifies certain pixels is related to the accuracy of those pixels, suggesting that it can be used to generate pixel-level accuracy estimates.

\fullcite{witjes2024iterative} (In review)

\textbf{Chapter 5} builds on the methods and findings of the previous chapters. 
Firstly, it presents a workflow to create larger and more detailed training datasets from multiple open data sources that are compatible with the hierarchical LUCAS land cover legend. 
Secondly, it investigates to what extent iteration classification uncertainty (ICU) can be used to provide reliable pixel-based uncertainty for land cover maps. It does this by creating high thematic resolution land cover maps of several NUTS2 regions across Europe, using Eurostat area estimates, training data extracted and harmonized from CORINE and EuroCrops, and validating them with LUCAS land cover points. 
The pixel-based error estimates are then used to aggregate uncertain predictions to more general classes and guarantee a baseline level of accuracy at the cost of some thematic detail. Different heuristics of prediction uncertainty are compared in their reliability to guarantee an error rate and the extent to which thematic detail must be sacrificed.

\fullcite{witjes2024hierarchical}
(Work in progress)